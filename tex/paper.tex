%
% Copyright (c) Meta Platforms, Inc. and affiliates.
%
% This source code is licensed under the MIT license found in the LICENSE file
% in the root directory of this source tree. 
%

% SIAM Article Template
\documentclass[hidelinks,onefignum,onetabnum]{siamart220329}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{shared}
\usepackage[algoruled]{algorithm2e}
%%%%\SetAlgoSkip{smallskip}
%%%%\SetAlgoInsideSkip{smallskip}
%%%%\SetAlCapFnt{\small}
%%%%\SetAlCapNameFnt{\small}
%%%%\setlength{\interspacetitleruled}{\smallskipamount}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={An efficient algorithm for integer lattice reduction},
  pdfauthor={Fran\c{c}ois Charton, Kristin Lauter, Cathy Li, and Mark Tygert}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument[][nocite]{supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
A lattice of integers is the collection of all linear combinations
of a set of vectors for which all entries of the vectors are integers
and all coefficients in the linear combinations are also integers.
Lattice reduction refers to the problem of finding a set of vectors
in a given lattice such that the collection of all integer linear combinations
of this subset is still the entire original lattice and so that
the Euclidean norms of the subset are reduced.
The present paper proposes simple, efficient iterations for lattice reduction
which are guaranteed to reduce the Euclidean norms of the basis vectors
(the vectors in the subset) monotonically during every iteration.
Each iteration selects the basis vector for which projecting off
(with integer coefficients) the components of the other basis vectors
along the selected vector minimizes the Euclidean norms
of the reduced basis vectors.
Each iteration projects off the components along the selected basis vector
and efficiently updates all information required for the next iteration
to select its best basis vector and perform the associated projections.
\end{abstract}

% REQUIRED
\begin{keywords}
matrix, projection, orthogonalization, optimization, cryptography,
number theory
\end{keywords}

% REQUIRED
\begin{MSCcodes}
65F25, 65K10, 11T71
\end{MSCcodes}



\section{Introduction}
\label{intro}

Lattices of integers are common tools in cryptography and number theory,
among other areas, as reviewed by~\cite{cassels}, \cite{peikart}, and others.
A lattice of integers in $m$ dimensions consists
of all linear combinations of a set of $n$ vectors,
with all coefficients in the linear combinations being integers
and all $m$ entries of each of the $n$ vectors being integers.
Thus, the lattice is a collection of infinitely many vectors of integers.

The set of $n$ vectors whose linear combinations form the lattice
is known as a ``basis'' for the lattice.
Traditionally the vectors in the basis are required to be linearly independent,
but throughout the present paper, the term ``basis'' will abuse terminology
slightly in omitting any requirement to be linearly independent.
The term ``reduced basis'' refers
to any unimodular integer linear transformation of a basis.
(A unimodular integer linear transformation is multiplication
with an invertible $n \times n$ matrix such that all entries of the matrix
and its inverse are integers; the entries of the inverse of a matrix
of integers are integers if and only if the absolute value
of the determinant of the matrix is 1 --- this follows straightforwardly
from the Cramer Rule for solving systems of linear equations.)
The goal of lattice reduction (the topic of the present paper)
is to minimize the Euclidean norms of the vectors in the reduced basis.

Finding a nonzero vector in the lattice whose Euclidean norm is least
(or nearly least) is a common use of lattice reduction. To see the connection,
fix any positive real number $p$.
Any reduced basis which minimizes the sum of the $p$-th powers
of the Euclidean norms of the vectors in the reduced basis
must include a shortest vector as one of the basis vectors.
Indeed, if no basis vector is the shortest vector,
then the sum of the $p$-th powers of the Euclidean norms
can be made smaller by replacing with the shortest vector
one of the basis vectors whose projection on the shortest vector is nonzero.
``Projection'' is defined as follows.

The projection of a column vector $v$ onto a column vector $w$ is $c \cdot w$,
with the scalar coefficient $c$ defined to be the real number
%
\begin{equation}
c = \frac{w^\top v}{w^\top w},
\end{equation}
%
where $w^\top$ denotes the transpose of $w$.
Needless to say, $w^\top v$ is the inner product between $w$ and $v$,
while $w^\top w$ is the inner product of $w$ with itself, which is of course
the square of the Euclidean norm of $w$.

Unfortunately, even if all entries of both $v$ and $w$ are integers,
subtracting off this projection $c \cdot w$ from $v$ directly
would result in a vector, $v - c \cdot w$, whose entries may not be integers.
Fortunately, all entries of the vector $v - \nint(c) \cdot w$
would remain integers, and we prove in Subsection~\ref{theory} below that
the Euclidean norm of $v - \nint(c) \cdot w$ is less than or equal to
the Euclidean norm of $v$. (Here and throughout the present paper,
$\nint(c)$ denotes the result of rounding $c$ to the nearest integer.
The reader is welcome to use any convention desired for the definition
of $\nint(k + 1/2)$, where $k$ is an integer, provided that the convention
is used consistently. The proofs in Subsection~\ref{theory} below omit
special consideration of these special cases, as the proofs are trivial
in such cases.) In fact, if both $v$ and $w$ are in a lattice of integers,
then $v - \nint(c) \cdot w$ is in the same lattice of integers
and is shorter (or at least no longer) than $v$.
Theorem~\ref{theorem} in Subsection~\ref{theory} below elaborates.
Mapping the pair $v$ and $w$ to the pair $v - \nint(c) \cdot w$ and $w$
is clearly a unimodular integer linear transformation,
as is any composition of such mappings
(since the product of unimodular integer linear transformations
is just another unimodular integer linear transformation).

Let us denote by $a^0_1$, $a^0_2$, \dots, $a^0_n$ the initial basis vectors,
each being a column vector of $m$ integers.
The present paper proposes an iterative algorithm that reduces the basis
from iteration $i$ to iteration $i+1$ from $a^i_1$, $a^i_2$, \dots, $a^i_n$
to $a^{i+1}_1$, $a^{i+1}_2$, \dots, $a^{i+1}_n$, starting with $i=0$.
Each iteration $i$ of the algorithm selects an index $k$
that minimizes the Euclidean norms $\| a^i_j - c^i_{j,k} \cdot a^i_k \|$
for all $j \ne k$, where the scalar projection coefficient $c^i_{j,k}$ is
%
\begin{equation}
\label{rounded}
c^i_{j,k} = \nint\left( \frac{(a^i_k)^\top a^i_j}{(a^i_k)^\top (a^i_k)} \right)
\end{equation}
%
for $j \ne k$ and
%
\begin{equation}
c^i_{j,k} = 0
\end{equation}
%
for $j = k$.
Specifically, the index $k$ is chosen to minimize the sum of the $p$-th powers
of the Euclidean norms, that is, $k$ minimizes
$\sum_{j = 1}^n \| a^i_j - c^i_{j,k} \cdot a^i_k \|^p$.
(If $p = 2$, then this sum is the square of the Frobenius norm of the matrix.)
Iteration $i$ constructs the matrix for the next iteration as the matrix whose
columns are the basis vectors $a^{i+1}_j = a^i_j - c^i_{j,k} \cdot a^i_k$.

{\it Prima facie}, considering all possible indices in order to minimize
the sum of the $p$-th powers appears to be computationally expensive.
However, the sum being minimized can be evaluated efficiently
via the Gram matrix whose entries are $g^i_{j,k} = (a^i_k)^\top a^i_j$,
as can the projection coefficients $c^i_{j,k}$ defined in~(\ref{rounded}).
Moreover, updating the entries of the Gram matrix from one iteration
to the next is also computationally efficient.

The algorithm of the present paper can complement others for lattice reduction,
such as the ``LLL'' algorithm introduced by~\cite{lenstra-lenstra-lovasz}.
Two entire books about the LLL algorithm are those
of~\cite{nguyen-vallee} and~\cite{bremner}.
The numerical examples presented in Section~\ref{results} below report
the results of running the algorithm of~\cite{lenstra-lenstra-lovasz}
in conjunction with the algorithm of the present paper.
The implementation of the LLL algorithm used here is
significantly more efficient (albeit less robust to worst-case,
adversarial examples devised for applications outside cryptography)
than the others' reviewed by~\cite{schnorr-euchner} and~\cite{stehle};
see Subsection~\ref{implementation} below
for details.\footnote{Permissively licensed open-source codes implementing
both the algorithm of the present paper and the classical LLL algorithm
of~\cite{lenstra-lenstra-lovasz} are available
at \url{https://github.com/facebookresearch/latticer}}

The primary purpose of the algorithm of the present paper is to polish
the outputs of other algorithms for lattice reduction;
on its own, the algorithm of the present paper tends to get stuck
in local minima, with the iterations attaining a fixed-point equilibrium
that is far away from optimally minimizing the sum of the $p$-th powers.
The convergence is monotonic, but need not arrive at the optimal minimum
when fully converged.

The remainder of the present paper has the following structure:
Section~\ref{methods} describes and analyzes the algorithm in detail. 
Section~\ref{results} illustrates the algorithm via several numerical examples,
comparing and combining the proposed algorithm with the classic method
of~\cite{lenstra-lenstra-lovasz}.
Section~\ref{conclusion} draws some conclusions.
Section~\ref{poor} of the Supplementary Materials
sketches several seemingly natural alternatives
that performed poorly in numerical experiments;
this supplementary section also points to other authors' variations
of the LLL algorithm.
Section~\ref{further} of the Supplementary Materials complements the figures
of Section~\ref{results} with further figures.



\section{Methods}
\label{methods}

This section elaborates the algorithm of the present paper.
First, Subsection~\ref{algorithm} details the algorithm and estimates
its computational costs.
Then, Subsection~\ref{theory} proves that the algorithm
converges monotonically, strictly reducing the sum of the $p$-th powers
of the Euclidean norms of the basis vectors during every iteration
(and hence halting after a finite number of iterations).


\subsection{Algorithm}
\label{algorithm}

This subsection describes the algorithm of the present paper in detail.
Pseudocode is available in Algorithm~\ref{ours}.

Consider the $n$ column vectors $a^0_1$, $a^0_2$, \dots, $a^0_n$,
each of size $m \times 1$.
The proposed scheme makes no assumption on the relative sizes of $m$ and $n$;
that is, $m$ can be less than, equal to, or greater than $n$.
Calculate the entries of the symmetric square Gram matrix 
%
\begin{equation}
\label{Gram0}
g^0_{j,k} = (a^0_k)^\top (a^0_j)
\end{equation}
%
for $j = 1$, $2$, \dots, $n$, and $k = 1$, $2$, \dots, $n$,
where $(a^0_k)^\top$ denotes the transpose of $a^0_k$.
(Of course, $(a^0_k)^\top (a^0_j)$ is simply the inner product
between $a^0_k$ and $a^0_j$.)
This costs $\bigoh(mn^2)$ operations.

Iterations, $i = 0$, $1$, $2$, \dots, will maintain the relation
%
\begin{equation}
\label{Gram}
g^i_{j,k} = (a^i_k)^\top (a^i_j)
\end{equation}
%
for $j = 1$, $2$, \dots, $n$, and $k = 1$, $2$, \dots, $n$.

Now repeat all of the following steps, again and again,
moving from $i = 0$ to $i = 1$ to $i = 2$ and so on:

Calculate the projection coefficients
%
\begin{equation}
c^i_{k,k} = 0
\end{equation}
%
for $k = 1$, $2$, \dots, $n$, and
%
\begin{equation}
\label{coeffs}
c^i_{j,k} = \nint\left( \frac{(a^i_k)^\top a^i_j}
                             {(a^i_k)^\top (a^i_k)} \right)
          = \nint\left( \frac{g^i_{j,k}}{g^i_{k,k}} \right)
\end{equation}
%
for $j = 1$, $2$, \dots, $n$, and $k = 1$, $2$, \dots, $n$ with $k \ne j$,
where $\nint$ denotes the nearest integer.
This costs $\bigoh(n^2)$ operations.

The sum of the $p$-th powers of the Euclidean norms
when projecting off the $k$-th vector is
%
\begin{equation}
\label{sos}
s^i_k = \sum_{j=1}^n \| a^i_j - c^i_{j,k} \, a^i_k \|^p
= \sum_{j=1}^n \left( g^i_{j,j} + (c^i_{j,k})^2 \, g^i_{k,k}
- 2 c^i_{j,k} \, g^i_{j,k} \right)^{p/2}
\end{equation}
%
for $k = 1$, $2$, \dots, $n$.
This costs $\bigoh(n^2)$ to compute.

Define $\tilde{\imath}$ to be the index such that $s^i_{\tilde{\imath}}$
is minimal, that is,
%
\begin{equation}
\tilde{\imath} = \argmin_{1 \le k \le n} s^i_k.
\end{equation}
%
This costs $\bigoh(n)$ to calculate.

Project off the $\tilde{\imath}$-th vector to update every vector
%
\begin{equation}
\label{ortho0}
a^{i+1}_k
= a^i_k - c^i_{k,\tilde{\imath}} \, a^i_{\tilde{\imath}}
\end{equation}
%
for $k = 1$, $2$, \dots, $n$, that is,
%
\begin{equation}
\label{ortho}
(a^{i+1}_k)_j
= (a^i_k)_j - c^i_{k,\tilde{\imath}} \, (a^i_{\tilde{\imath}})_j
\end{equation}
%
for $j = 1$, $2$, \dots, $m$, and $k = 1$, $2$, \dots, $n$,
where $(a^i_k)_j$ denotes the $j$-th entry of $a^i_k$.
This costs $\bigoh(mn)$.

Update the Gram matrix via the relation
%
\begin{equation}
\label{Gram_update}
g^{i+1}_{j,k}
= (a^{i+1}_k)^\top (a^{i+1}_j)
= (a^i_k - c^i_{k,\tilde{\imath}} \, a^i_{\tilde{\imath}})^\top
  (a^i_j - c^i_{j,\tilde{\imath}} \, a^i_{\tilde{\imath}})
= g^i_{j,k}
+ c^i_{j,\tilde{\imath}} \, c^i_{k,\tilde{\imath}}
\, g^i_{\tilde{\imath},\tilde{\imath}}
- c^i_{j,\tilde{\imath}} \, g^i_{\tilde{\imath},k}
- c^i_{k,\tilde{\imath}} \, g^i_{j,\tilde{\imath}}
\end{equation}
%
for $j = 1$, $2$, \dots, $n$, and $k = 1$, $2$, \dots, $n$.
This costs $\bigoh(n^2)$.

The total cost per iteration is $\bigoh(mn+n^2)$ operations.
Notice that only the precomputation of the Gram matrix in~(\ref{Gram0})
and~(\ref{ortho}) explicitly involve the individual entries of the vectors;
all other steps in the iterations involve only the entries of the Gram matrix.

\begin{remark}
Another possibility is to replace the sum in~(\ref{sos})
with a maximum, replacing~(\ref{sos}) with
%
\begin{equation}
s^i_k = \max_{1 \le j \le n} \| a^i_j - c^i_{j,k} \, a^i_k \|^2
= \max_{1 \le j \le n} \left( g^i_{j,j} + (c^i_{j,k})^2 \, g^i_{k,k}
- 2 c^i_{j,k} \, g^i_{j,k} \right)
\end{equation}
%
for $k = 1$, $2$, \dots, $n$.
However, using the maximum may fail to force any but the longest vectors
in the reduced basis to become shorter.
\end{remark}

\begin{remark}
The Gram matrix is symmetric, that is, $g^i_{j,k} = g^i_{k,j}$
for all $i = 0$, $1$, $2$, \dots,
for $j = 1$, $2$, \dots, $n$, and $k = 1$, $2$, \dots, $n$.
Calculating $g^i_{j,k}$ for only $j \le k$ suffices to fill the entire matrix
for all $j = 1$, $2$, \dots, $n$, and $k = 1$, $2$, \dots, $n$.
This can save computational costs in~(\ref{Gram0}) and~(\ref{Gram_update}).
\end{remark}

\begin{remark}
Cryptography can benefit from reductions to collections of basis vectors
that include all the unit basis vectors, each multiplied by the prime order
of a finite field, in addition to the other basis vectors.
This essentially formalizes the concept of lattice reduction
over the finite field. 
Instead of working directly on a collection of basis vectors
$\left( \begin{array}{c|c} A & q \cdot \Id \end{array} \right)$
in this way, where the columns of $A$ form the initial collection
of basis vectors, $q$ is the order of the finite field,
and $\Id$ is the identity matrix, the classical methods for lattice reduction
--- such as the LLL algorithm of~\cite{lenstra-lenstra-lovasz} ---
must operate instead on 
%
\begin{equation}
\left( \begin{array}{c|c} A   & q \cdot \Id \\\hline
                          \Id & 0           \end{array} \right),
\end{equation}
%
effectively doubling the dimension of the basis vectors.
\end{remark}


\subsection{Theory}
\label{theory}

The purpose of this subsection is to state and prove Theorem~\ref{theorem},
elaborating the facts stated in the fifth paragraph of the introduction,
Section~\ref{intro}.

The following lemma is helpful in the proof of the subsequent lemma.
%
\begin{lemma}
Suppose that $r$ is a real number. Then
%
\begin{equation}
\label{simplified}
(\nint(r))^2 - 2 \nint(r) \, r \le 0.
\end{equation}
%
\end{lemma}

\begin{proof}
Since the sign of $\nint(r)$ is the same as the sign of $r$ (or 0),
the sign of the left-hand side of~(\ref{simplified}) is the sign of $r$ (or 0)
times the sign of
%
\begin{equation}
\label{final}
\nint(r) - 2 r.
\end{equation}
%
Now, the sign of~(\ref{final}) is opposite to the sign of $r$:
if $r > 1/2$, then $\nint(r) - 2r \le r+1/2-2r = 1/2-r < 0$;
if $r < -1/2$, then $\nint(r) - 2r \ge r-1/2-2r = -1/2-r > 0$;
if $|r| < 1/2$, then $\nint(r) - 2r = -2r$, whose sign is opposite to $r$'s.
Combining these observations yields~(\ref{simplified}).
\end{proof}


The following lemma is helpful in the proof of the subsequent theorem.
%
\begin{lemma}
Suppose that $c^i_{j,k}$ is the coefficient defined in~(\ref{coeffs})
and $g^i_{j,k}$ is the entry of the Gram matrix defined in~(\ref{Gram})
for $i = 0$, $1$, $2$, \dots, $j = 1$, $2$, \dots, $n$,
and $k = 1$, $2$, \dots, $n$. Then
%
\begin{equation}
\label{update}
(c^i_{j,k})^2 \, g^i_{k,k} - 2 c^i_{j,k} \, g^i_{j,k} \le 0
\end{equation}
%
for $i = 0$, $1$, $2$, \dots, $j = 1$, $2$, \dots, $n$,
and $k = 1$, $2$, \dots, $n$.
\end{lemma}

\begin{proof}
Combining~(\ref{coeffs}) and the fact that
$g^i_{k,k} = (a^i_k)^\top (a^i_k) \ge 0$
yields that the sign of the left-hand side of~(\ref{update})
is the same as the sign of
%
\begin{equation}
(c^i_{j,k})^2 - 2 c^i_{j,k} \, \frac{g^i_{j,k}}{g^i_{k,k}}
= (\nint(f^i_{j,k}))^2 - 2 \nint(f^i_{j,k}) \, f^i_{j,k},
\end{equation}
%
where
%
\begin{equation}
f^i_{j,k} = \frac{g^i_{j,k}}{g^i_{k,k}}.
\end{equation}
%
The lemma follows from~(\ref{simplified}) with $r = f^i_{j,k}$.
\end{proof}


The following theorem states that projection using the coefficients
defined in~(\ref{coeffs}) never increases the Euclidean norm.
This implies that the above algorithm never increases the Euclidean norm
of any of the basis vectors at any time; the sum of the $p$-th powers
of the Euclidean norms therefore converges monotonically to a local minimum,
for every possible (positive) value of $p$ simultaneously.
%
\begin{theorem}
\label{theorem}
Suppose that $c^i_{j,k}$ is the coefficient defined in~(\ref{coeffs})
for $i = 0$, $1$, $2$, \dots, $j = 1$, $2$, \dots, $n$,
and $k = 1$, $2$, \dots, $n$.
Then the Euclidean norms satisfy
%
\begin{equation}
\label{mono}
\| a^i_j - c^i_{j,k} \, a^i_k \| \le \| a^i_j \|
\end{equation}
%
for $i = 0$, $1$, $2$, \dots, $j = 1$, $2$, \dots, $n$,
and $k = 1$, $2$, \dots, $n$.
\end{theorem}

\begin{proof}
According to~(\ref{Gram}), the square of the right-hand side of~(\ref{mono}) is
%
\begin{equation}
\label{rhs}
\| a^i_j \|^2 = g^i_{j,j}.
\end{equation}
%
The square of the left-hand side of~(\ref{mono}) is
%
\begin{equation}
\label{lhs}
\| a^i_j - c^i_{j,k} \, a^i_k \|^2
= g^i_{j,j} + (c^i_{j,k})^2 \, g^i_{k,k} - 2 c^i_{j,k} \, g^i_{j,k}.
\end{equation}
%
Formula~(\ref{update}) shows that~(\ref{lhs})
is less than or equal to~(\ref{rhs}).
\end{proof}



\section{Results and Discussion}
\label{results}

This section presents the results
of several numerical experiments.\footnote{Permissively licensed open-source
software that automatically reproduces all results reported
in the present paper is available at
\url{https://github.com/facebookresearch/latticer}}
Subsection~\ref{figures} describes the figures.
Subsection~\ref{examples} constructs the examples whose numerical results
the figures report.
Subsection~\ref{implementation} provides details of the implementation
and computer system used.
Subsection~\ref{simple} describes the tables for a dead-simple example included
due to a reviewer's request.
Subsection~\ref{discussion} discusses the empirical results.

\subsection{Description of the figures}
\label{figures}

This subsection describes empirical results, illustrating the algorithms
via Figures~\ref{p2time1-1e-15}--\ref{p2err1-1e-15-31}.
Supplementary figures are available in Section~\ref{further}
of the Supplementary Materials.

The figures refer to the algorithm of~\cite{lenstra-lenstra-lovasz} as ``LLL.''
Subsection~\ref{implementation} details the especially efficient implementation
used here. The figures refer to the algorithm of the present paper as ``ours.''

The figures report on two kinds of experiments.
The first kind runs the LLL algorithm followed by ours,
once in each of ten trials. Each of the ten trials permutes the basis vectors
at random, with a different random permutation.
(The algorithm of the present paper is invariant to the ordering
of the basis vectors, whereas the LLL algorithm is sensitive to the ordering.)
The points plotted in the figures are the means of the ten trials.
So, for example, the times reported in Figure~\ref{p2time1-1e-15}
are averages over ten trials.
The plotted bars range from the associated minimum over the ten trials
to the associated maximum (not displaying the standard deviation
that error bars sometimes report).
Figures~\ref{p2err1-1e-15-13once} and~\ref{p2err1-1e-15-31once}
include such bars ranging from the minimum to the maximum of the ten trials.
The figures refer to this single randomized permutation followed by LLL
followed by ours as ``run once.''
Figures~\ref{p2time1-1e-15}, \ref{p2err1-1e-15-13once},
and~\ref{p2err1-1e-15-31once} concern this first kind of experiment.

The second kind of experiment runs the following sequence ten times
in succession: (1) random permutation of the basis vectors
followed by (2) LLL followed by (3) the algorithm of the present paper,
with each of the first nine times feeding its output into the input
of the next time. Algorithm~\ref{combination} formalizes this series
of ten three-step sequences explicitly. The figures refer to the sequence
of ten as ``run repeatedly.'' Figures~\ref{p2err1-1e-15-13}
and~\ref{p2err1-1e-15-31} concern this second kind of experiment.
The first steps (random permutation) affect the LLL algorithm,
which is sensitive to the ordering of the basis vectors
(while the algorithm of the present paper is invariant to the ordering).
The points plotted in Figures~\ref{p2err1-1e-15-13} and~\ref{p2err1-1e-15-31}
are the final results of the series of ten three-step sequences
(whereas the points plotted in Figures~\ref{p2time1-1e-15},
\ref{p2err1-1e-15-13once}, and~\ref{p2err1-1e-15-31once}
are the means over ten independent trials).

Figure~\ref{p2time1-1e-15} displays the time taken per trial.
Figures~\ref{p2err1-1e-15-13once}--\ref{p2err1-1e-15-31} report
the reduction either in the Frobenius norm
of the matrix of basis vectors or in the minimum of the Euclidean norms
of the basis vectors. (The Frobenius norm is the square root of the sum
of the squares of the entries of the matrix.)
The figures report the reduction due to LLL run on its own,
as well as the further reduction due to post-processing
the output from LLL via the algorithm of the present paper.
Therefore, the full reduction in norm is the product
of the reported fractions remaining after reduction,
as all runs polish the outputs of the LLL algorithm via ours.
The fraction reduced by ours that is reported for the sequence run repeatedly
pertains only to the very last run of ours,
whereas the fraction reduced by LLL reported
for the same sequence run repeatedly pertains to the entire series of ten runs
of LLL followed by ours, aside from separating out the very last run of ours.
The plots displaying the fraction reduced by the last run of ours
that is reported for the sequence run repeatedly feature titles,
``reduction in Frob.\ norm by ours after reps.\ of ours+LLL''
and ``reduction in min.\ norm by ours after reps.\ of ours+LLL,''
abbreviating ``repetitions'' to ``reps.''
In all the figures, the plots on the right display the results
of running the algorithm of Section~\ref{methods}
(that is, Algorithm~\ref{ours}) after having run either the LLL algorithm
or all of Algorithm~\ref{combination} except for the final run
of the algorithm of Section~\ref{methods} (which is Algorithm~\ref{ours}).

The horizontal axes report the dimension $n$ of the matrix
whose columns are the initial basis vectors being reduced
(the matrix is $n \times n$).
The figure captions' $\delta$ is that used
in the so-called ``Lov\'asz criterion'' of the LLL algorithm.
The figure captions' $p$ is the power used in~(\ref{sos})
for the algorithm of the present paper.
The figure captions' $q$ is an integer characterizing the size of the entries
of the initial basis vectors prior to reduction;
the following subsection provides further details about $q$.

Section~\ref{further} of the Supplementary Materials presents further figures,
reporting results for different values of $\delta$ and $p$.
The conclusions that can be drawn from the further figures
appear to be broadly consistent with those presented in the present section.

\subsection{Description of the examples}
\label{examples}

This subsection details the particular examples whose numerical results
the figures of the previous subsection report.

To construct the initial basis vectors being reduced
as related to a finite field whose order is a large prime integer $q$,
the examples consider $q = 2^{13} - 1$ and $q = 2^{31} - 1$
(two well-known Mersenne primes).
The matrix whose columns are the initial basis vectors
for the figures' examples takes the form
%
\begin{equation}
\label{matexample}
A = \left( \begin{array}{c|c} R & q \cdot \Id \\\hline
                              \Id & 0 \end{array} \right),
\end{equation}
%
where ``$\Id$'' denotes the identity matrix,
``$0$'' denotes the matrix whose entries are all zeros,
and $R$ denotes the pseudorandom matrix whose entries are drawn
independent and identically distributed from the uniform distribution
over the integers $-(q-1)/2$, $-(q-3)/2$, \dots, $(q-3)/2$, $(q-1)/2$.
This special form of matrix effectively views the entries of $R$ modulo $q$,
as unimodular integer linear transformations acting on $A$ from the right
can add any integer multiple of $q$ to any entry of $R$.
The captions to the figures give the values of $q$ considered.

The dimension of the matrix $R$ in~(\ref{matexample}) is
$(2 \ell) \times \ell$, where $\ell = 2$, $4$, $8$, \dots, $128$.
Hence the dimension of the matrix $A$ in~(\ref{matexample}) whose columns
are the initial basis vectors is $n \times n$, with $n = 3 \ell$,
so that $n = 6$, $12$, $24$, \dots, $384$.
The horizontal axes of the figures give the values of $n$
associated with the corresponding points in the plots.

\subsection{Implementation details}
\label{implementation}

This subsection details the implementations
(including significant optimizations) used in the reported results.

The implementation of the LLL algorithm of~\cite{lenstra-lenstra-lovasz}
used here is entirely in IEEE standard double-precision arithmetic,
with acceleration via basic linear algebra subroutines (BLAS)
and enhanced accuracy via re-orthogonalization.
BLAS originates from~\cite{lawson-hanson-kincaid-krogh},
\cite{blas}, and others, with implementation in Apple's Accelerate Framework
of Xcode for MacOS Ventura 13.5 in the experiments reported here
(and compiled using Clang LLVM with the highest optimization flag,
{\tt -O3}, set). All experiments ran on a 2020 MacBook Pro
with a 2.3~GHz quad-core Intel Core i7 processor
and 3.733~GHz LPDDR4X SDRAM.
For the greatest portability, the implementation calls {\tt random()}
to generate random numbers; many compiler distributions
implement {\tt random()} via the BSD linear-congruential generator
(such compilers include our implementation's defaults
of MacOS Clang referenced by {\tt gcc} under Xcode
and GNU {\tt gcc} under Linux). The results reported here pertain
to this class of pseudorandom numbers.

Re-orthogonalization combats round-off errors,
as reviewed by~\cite{leon-bjorck-gander}.
Namely, all orthogonalization gets repeated until only at most the last digit
of any result changes, and then one additional round of orthogonalization
occurs (just to be sure).

Furthermore, upon any swapping of basis vectors in the LLL algorithm,
the implementation recomputes the swapped orthogonal basis vectors from scratch
for numerical stability. Recomputing from scratch incurs
extra computational expense, but no more than the computational cost incurred
by the size reduction which accompanies every swap.

The combination of re-orthogonalization and recomputing from scratch
swapped orthogonal basis vectors may not be sufficient to handle
the hardest possible examples constructed purposefully to test resistance
to round-off errors, but is sufficient to handle the random examples
of interest in cryptography (since cryptosystems necessarily
must generate bases at random in order to be secure).
Methods that take advantage of floating-point arithmetic
while also being able to tackle worst-case adversarial examples
are overviewed by~\cite{schnorr-euchner} and~\cite{stehle}.
For the more limited scope of cryptographic applications, however,
the enhanced accuracy of re-orthogonalization and recomputing from scratch
swapped orthogonal basis vectors enables all computations to leverage BLAS
for dramatic accelerations. The implementation of the algorithm
of the present paper also takes advantage of this acceleration,
primarily for calculating the initial Gram matrix
(other steps of the algorithm benefit less from BLAS than LLL does,
at least in the implementation reported here).

Figure~\ref{p2time1-1e-15} indicates that both LLL and the algorithm
of the present paper have running-times proportional to roughly $n^3$,
with LLL being over an order of magnitude slower.
The cost per iteration is proportional to $n^2$, as discussed
in Section~\ref{methods}; the numbers of iterations required appear
to be proportional to $n$ for the examples reported here.

\subsection{Description of the tables}
\label{simple}

This subsection describes empirical results from a dead-simple example,
illustrating the algorithms via Tables~\ref{simplefrob} and~\ref{simplemin}.

To keep everything as simple as possible, the tables report results
from processing a single $500 \times 500$ matrix whose entries
are independent and identically distributed pseudorandom numbers
drawn uniformly from the integers $0$, $1$, $2$, \dots, $q-1$,
where $q$ is the Mersenne prime specified in Tables~\ref{simplefrob}
and~\ref{simplemin}. The processing for the tables applies
either the LLL algorithm or ours (as indicated in the tables)
directly to the matrix, not combining any of the algorithms.
Table~\ref{simplefrob} reports the attained reduction in the Frobenius norm
of the matrix; Table~\ref{simplemin} reports the attained reduction
in the minimum Euclidean norm of any column (that is, of any vector
in the reduced basis). The tables also report the parameter $\delta$ used
in the so-called ``Lov\'asz criterion'' of the LLL algorithm;
for this example, we set the parameter $p$ used in our algorithm to $p = 2$.
In all cases, the LLL algorithm reduces the norms significantly less than ours.
Thus the LLL algorithm appears to be uniformly and significantly inferior
to ours on this random example (however, this example may be too simplistic
to be representative of applications to cryptography, despite fulfilling
a request from a reviewer).

\subsection{Discussion}
\label{discussion}

This subsection discusses the numerical results reported in the figures
and tables.

As mentioned in the last paragraph of Subsection~\ref{implementation},
the cost per iteration of either the LLL algorithm or ours is proportional
to $n^2$. Figure~\ref{p2time1-1e-15} indicates that both LLL and ours
run in time proportional to roughly $n^3$
(with LLL being over an order of magnitude slower),
in accord with the numbers of iterations taken to reach equilibrium
(that is, a fixed point) being proportional to the dimension $n$
in the experiments of the present section.
(Figure~\ref{p2time1-1e-1} in the Supplementary Materials shows the same,
but for $\delta = 1 - 10^{-1}$ rather than $\delta = 1 - 10^{-15}$,
where $\delta$ is the parameter in the so-called ``Lov\'asz criterion''
of the LLL algorithm.)

Figures~\ref{p2err1-1e-15-13once}--\ref{p2err1-1e-15-31}
indicate that both the LLL algorithm and ours reduce norms
as a function of the dimension $n$ similarly for different sizes
of the entries of the basis vectors; specifically, changing $q = 2^{13} - 1$
to $q = 2^{31} - 1$ has little effect on how the results vary
as a function of $n$. Repeated runs that could in principle help jump out
of local minima appear little more effective than simply taking the best
of several single runs (with each run randomizing the order
of the basis vectors differently).

In all cases except for the simplistic example from Subsection~\ref{simple},
LLL reduces norms far more than ours.
The algorithm of the present paper is suitable only for polishing the results
of another algorithm (such as LLL); the algorithm of the present paper
is very fast and often reduces norms beyond what other algorithms achieve,
but is ineffective on its own for anything other than demonstrating
that a given basis can be reduced further.
Ours appears to be more useful for reducing the Frobenius norm
than the minimum of the Euclidean norms of the basis vectors,
though the algorithm does reduce both (or at least never increases either,
as guaranteed by the theory of Subsection~\ref{theory}
and illustrated in all the figures and tables).

Possible applications include the following:
%
\begin{enumerate}
\item The algorithm of the present paper can often give a constructive proof
that the result of another algorithm for lattice reduction is not optimal.
\item Some lattice-based cryptosystems can be compromised with reductions
in the Euclidean norm of the shortest vector in the basis by
$\frac{100}{n}$ percent, where $n = m$ is both the number and dimension
of the basis vectors. For example, \cite{regev} mentions several such schemes.
In conjunction with other methods, the algorithm of the present paper
might help attack such cryptosystems.
\item Improvement via lattice reduction of the solution to Diophantine
equations and other problems from number theory can be of theoretical interest.
\item Recent work of~\cite{li-sotakova-wenger-allen-zhu-charton-lauter}
accelerated its preprocessing by a factor of 40 using the algorithm
of the present paper interleaved with its earlier method for lattice reduction.
Similar interleaving with the present paper's implementation
of the LLL algorithm could enable further acceleration.
\end{enumerate}

Section~\ref{further} of the Supplementary Materials
presents several more figures, with different settings of parameters;
all further corroborate the results discussed in the present subsection.
And, naturally, the algorithm and its analysis generalize to lattices formed
from linear combinations of vectors whose entries are real numbers
(not necessarily integers), with the coefficients in the linear combinations
being integers. However, the case involving real numbers may pose challenges
due to round-off errors.

The dual of a lattice of integers is in general such a lattice
of real vectors. The least-possible maximum of the Euclidean norms
of the vectors in a basis for the dual lattice yields bounds
on the Euclidean norm of the shortest nonzero vector in the original lattice,
courtesy of the transference of~\cite{banaszczyk},
as highlighted by~\cite{regev} and others.
The algorithm of the present paper can reduce the maximum
of the Euclidean norms toward the least possible.

\begin{figure}
\begin{centering}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot00t_lll.pdf}}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot00t_iterate.pdf}}

{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot10t_lll.pdf}}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot10t_iterate.pdf}}

\end{centering}
\caption{$\delta = 1-10^{-15}$, $p = 2$;
         the upper plots are for $q = 2^{13} - 1$,
         the lower plots are for $q = 2^{31} - 1$}
\label{p2time1-1e-15}
\end{figure}

\begin{figure}
\begin{centering}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot00frobmean1.pdf}}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot00frobmean2.pdf}}

{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot00minmean1.pdf}}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot00minmean2.pdf}}

\end{centering}
\caption{$\delta = 1-10^{-15}$, $p = 2$, $q = 2^{13} - 1$}
\label{p2err1-1e-15-13once}
\end{figure}

\begin{figure}
\begin{centering}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot00frobmulti1.pdf}}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot00frobmulti2.pdf}}

{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot00minmulti1.pdf}}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot00minmulti2.pdf}}

\end{centering}
\caption{$\delta = 1-10^{-15}$, $p = 2$, $q = 2^{13} - 1$}
\label{p2err1-1e-15-13}
\end{figure}

\begin{figure}
\begin{centering}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot10frobmean1.pdf}}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot10frobmean2.pdf}}

{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot10minmean1.pdf}}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot10minmean2.pdf}}

\end{centering}
\caption{$\delta = 1-10^{-15}$, $p = 2$, $q = 2^{31} - 1$}
\label{p2err1-1e-15-31once}
\end{figure}

\begin{figure}
\begin{centering}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot10frobmulti1.pdf}}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot10frobmulti2.pdf}}

{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot10minmulti1.pdf}}
{\includegraphics[width=0.495\textwidth]{../codes/1-1e-15/plot10minmulti2.pdf}}

\end{centering}
\caption{$\delta = 1-10^{-15}$, $p = 2$, $q = 2^{31} - 1$}
\label{p2err1-1e-15-31}
\end{figure}

\begin{table}
\caption{\noindent Frobenius norm of the matrix whose columns
are the basis vectors resulting from reducing the columns of a $500 \times 500$
matrix whose entries are independent and identically distributed pseudorandom
numbers drawn uniformly from the integers $0$, $1$, $2$, \dots, $q-1$}
\label{simplefrob}
\begin{centering}

\begin{tabular}{c|c|c|c|c}
\hline
 & & Frobenius norm & fraction remaining & fraction remaining \\
$\delta$ & $q$ & before reduction & after running LLL & after running ours \\
\hline\hline
$1-10^{-15}$ & $2^{13}-1$ & 2.36E06 & 0.805 & 0.667 \\\hline
$1-10^{-15}$ & $2^{31}-1$ & 6.19E11 & 0.822 & 0.667 \\\hline
 $1-10^{-1}$ & $2^{13}-1$ & 2.36E06 & 0.813 & 0.667 \\\hline
 $1-10^{-1}$ & $2^{31}-1$ & 6.19E11 & 0.815 & 0.667 \\\hline
\end{tabular}

\end{centering}
\end{table}

\begin{table}
\vspace{.1in}
\caption{\noindent Minimum Euclidean norm of any of the basis vectors resulting
from reducing the columns of a $500 \times 500$ matrix whose entries
are independent and identically distributed pseudorandom numbers drawn
uniformly from the integers $0$, $1$, $2$, \dots, $q-1$}
\label{simplemin}
\begin{centering}

\begin{tabular}{c|c|c|c|c}
\hline
 & & min.\ Euclidean & fraction remaining & fraction remaining \\
$\delta$ & $q$ & norm initially & after running LLL & after running ours \\
\hline\hline
$1-10^{-15}$ & $2^{13}-1$ & 1.00E05 & 0.675 & 0.658 \\\hline
$1-10^{-15}$ & $2^{31}-1$ & 2.62E10 & 0.675 & 0.658 \\\hline
 $1-10^{-1}$ & $2^{13}-1$ & 1.00E05 & 0.689 & 0.658 \\\hline
 $1-10^{-1}$ & $2^{31}-1$ & 2.62E10 & 0.689 & 0.658 \\\hline
\end{tabular}

\end{centering}
\end{table}



\LinesNumbered
\begin{algorithm}
\caption{Lattice reduction via the Gram matrix and projections}
\label{ours}
\DontPrintSemicolon
\KwIn{Positive integers $m$ and $n$, a positive real number $p$, and
column vectors $a^0_1$, $a^0_2$, \dots, $a^0_n$, each of size $m \times 1$.}
\KwOut{A positive integer $i$ and
column vectors $a^i_1$, $a^i_2$, \dots, $a^i_n$, each $m \times 1$,
such that the vectors are a unimodular integer linear transform
of the inputs $a^0_1$, $a^0_2$, \dots, $a^0_n$
and $\|a^i_k\| \le \|a^0_k\|$ for all $k = 1$, $2$, \dots, $n$.}

Set $g^0_{j,k} = (a^0_k)^\top (a^0_j)$
for $j = 1$, $2$, \dots, $n$, and $k = 1$, $2$, \dots, $n$.

Set $i = 0$.

\Repeat{$(a^i_k)_j = (a^{i-1}_k)_j$
for all $j = 1$, $2$, \dots, $m$, and all $k = 1$, $2$, \dots, $n$.}{

Set $c^i_{k,k} = 0$
for $k = 1$, $2$, \dots, $n$.

Set $c^i_{j,k} = \nint( g^i_{j,k} / g^i_{k,k} )$
for $j = 1$, $2$, \dots, $n$, and $k = 1$, $2$, \dots, $n$ with $k \ne j$.

Set $s^i_k
= \sum_{j=1}^n \left( g^i_{j,j} + (c^i_{j,k})^2 \, g^i_{k,k}
- 2 c^i_{j,k} \, g^i_{j,k} \right)^{p/2}$
for $k = 1$, $2$, \dots, $n$.

Set $\tilde{\imath} = \argmin_{1 \le k \le n} s^i_k$.

Set $(a^{i+1}_k)_j
= (a^i_k)_j - c^i_{k,\tilde{\imath}} \, (a^i_{\tilde{\imath}})_j$
for $j = 1$, $2$, \dots, $m$, and $k = 1$, $2$, \dots, $n$.

Set $g^{i+1}_{j,k}
= g^i_{j,k}
+ c^i_{j,\tilde{\imath}} \, c^i_{k,\tilde{\imath}}
\, g^i_{\tilde{\imath},\tilde{\imath}}
- c^i_{j,\tilde{\imath}} \, g^i_{\tilde{\imath},k}
- c^i_{k,\tilde{\imath}} \, g^i_{j,\tilde{\imath}}$
for $j = 1$, $2$, \dots, $n$, and $k = 1$, $2$, \dots, $n$.

Increment $i$ by 1.

}

\end{algorithm}


\LinesNumbered
\begin{algorithm}
\caption{Series of ten sequences, each having the same three steps:
(1) random permutation followed by
(2) the LLL algorithm of~\cite{lenstra-lenstra-lovasz} followed by
(3) Algorithm~\ref{ours}}
\label{combination}
\DontPrintSemicolon
\KwIn{A positive integer $n$, a positive real number $p$, and
column vectors $a^0_1$, $a^0_2$, \dots, $a^0_n$, each of size $n \times 1$.}
\KwOut{Column vectors $a^{10}_1$, $a^{10}_2$, \dots, $a^{10}_n$,
each $n \times 1$,
such that the vectors are a unimodular integer linear transformation
of the input vectors $a^0_1$, $a^0_2$, \dots, $a^0_n$.}

\For{i = 0, 1, 2, \dots, 9}{

Construct a uniformly random permutation
$\sigma^i_1$, $\sigma^i_2$, \dots, $\sigma^i_n$ of the integers
$1$, $2$, \dots, $n$ via the Fisher-Yates-Durstenfeld-Knuth shuffle
described in Section~3.4.2 of~\cite{knuth}.

Set $b^i_k = a^i_{\sigma^i_k}$ for $k = 1$, $2$, \dots, $n$.

Run the LLL algorithm of~\cite{lenstra-lenstra-lovasz}
(implemented as detailed in Subsection~\ref{implementation})
on $b^i_1$, $b^i_2$, \dots, $b^i_n$ to obtain
vectors $b^{i+1}_1$, $b^{i+1}_2$, \dots, $b^{i+1}_n$,
each $n \times 1$, such that $b^{i+1}_1$, $b^{i+1}_2$, \dots, $b^{i+1}_n$
are a unimodular integer linear transformation of the vectors
$b^i_1$, $b^i_2$, \dots, $b^i_n$.

Run Algorithm~\ref{ours} (with the given values of $n$ and $p$
and with $m = n$) on $b^{i+1}_1$, $b^{i+1}_2$, \dots, $b^{i+1}_n$
to obtain vectors $a^{i+1}_1$, $a^{i+1}_2$, \dots, $a^{i+1}_n$,
each $n \times 1$, such that $a^{i+1}_1$, $a^{i+1}_2$, \dots, $a^{i+1}_n$
are a unimodular integer linear transform of
$b^{i+1}_1$, $b^{i+1}_2$, \dots, $b^{i+1}_n$
and $\|a^{i+1}_k\| \le \|b^{i+1}_k\|$ for all $k = 1$, $2$, \dots, $n$.

}

\end{algorithm}



\section{Conclusion}
\label{conclusion}

The present paper proposes a very simple and efficient scheme
for lattice reduction. The scheme reduces the Euclidean norms
of the basis vectors monotonically, as guaranteed by rigorous proofs.
Fortunately, the algorithm of the present paper runs much faster
than even a highly optimized implementation of the most classical baseline,
the LLL algorithm of~\cite{lenstra-lenstra-lovasz}.
Unfortunately, the proposed algorithm reduces norms far less than LLL
if not used in conjunction with a method such as LLL.
The main use of the proposed scheme should therefore be to polish the outputs
of another algorithm (such as LLL).
On its own, the algorithm of the present paper tends to get stuck
in rather shallow local minima, with the iterations reaching
a fixed-point equilibrium that is far away from optimally minimizing
the Euclidean norms of the basis vectors. Convergence is monotonic
and hence guaranteed, but equilibrium tends to attain
without reaching the global optimum.
The algorithm of the present paper is extremely efficient computationally,
however, so can be suitable for rapidly burnishing the outputs
of other algorithms for lattice reduction.



\section*{Acknowledgements}

We would like to thank our colleagues, Zeyuan Allen-Zhu, Kamalika Chaudhuri,
Mingjie Chen, Evrard Garcelon, Matteo Pirotta, Jana Sotakova, and Emily Wenger.
We would also like to thank the anonymous reviewers and editor.



\clearpage



\bibliography{lattice}
\bibliographystyle{siamplain}



\end{document}
